{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oT5U8GeLzF8F"
   },
   "source": [
    "# transformer istrain\n",
    "\n",
    "Q: Does a Transformer know if it being trained? This has implications on AI safety.\n",
    "\n",
    "Hypothesis: dropout \"leaks\" the train/eval phase bit.\n",
    "\n",
    "Reference: https://twitter.com/karpathy/status/1635049541534879745"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUtAR0bTzBR3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_mX9k8G81wcy"
   },
   "outputs": [],
   "source": [
    "# repro\n",
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cz8TPWoEzS_0"
   },
   "outputs": [],
   "source": [
    "\n",
    "# create a toy transformer network doing BCE loss on last token\n",
    "C = 64 # num channels\n",
    "\n",
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super(TinyTransformer, self).__init__()\n",
    "        # random small encoder decoder transformer\n",
    "        self.transformer = nn.Transformer(d_model=C, nhead=4, \n",
    "                       num_encoder_layers=4, num_decoder_layers=4,\n",
    "                       dim_feedforward=C*4, dropout=dropout)\n",
    "        self.fc = nn.Linear(C, 1)\n",
    "    def forward(self, xe, xd):\n",
    "        # forward the transformer\n",
    "        x = self.transformer(xe, xd)\n",
    "        # select the last time step to make the prediction\n",
    "        x = x[:, -1, :]\n",
    "        # forward the classifier\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTEMpYIdzh8S"
   },
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # training loop\n",
    "    B, T = 8, 4\n",
    "    steps = 300\n",
    "\n",
    "    for n in range(steps):\n",
    "\n",
    "        # zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # phase 1: train mode\n",
    "        xe = torch.randn(B, T, C) # B,T,C for encoder\n",
    "        xd = torch.randn(B, T, C) # B,T,C for decoder\n",
    "        model.train()\n",
    "        x = model(xe, xd)\n",
    "        y = torch.ones(B, 1) # positive label: we are training\n",
    "        loss = F.binary_cross_entropy_with_logits(x, y)\n",
    "        loss.backward()\n",
    "        if n % 100 == 0 or n == steps-1:\n",
    "            print(f\"{n} loss in phase 1: {loss.item()}\")\n",
    "\n",
    "        # phase 2: eval mode\n",
    "        xe = torch.randn(B, T, C) # B,T,C for encoder\n",
    "        xd = torch.randn(B, T, C) # B,T,C for decoder\n",
    "        model.eval()\n",
    "        x = model(xe, xd)\n",
    "        y = torch.zeros(B, 1) # negative label: we are not training\n",
    "        loss = F.binary_cross_entropy_with_logits(x, y)\n",
    "        loss.backward()\n",
    "        if n % 100 == 0 or n == steps-1:\n",
    "            print(f\"{n} loss in phase 2: {loss.item()}\")\n",
    "\n",
    "        # update\n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYmJ5K9o1XFn"
   },
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "\n",
    "    # evaluate accuracy on some synthetic test data\n",
    "    corrects = []\n",
    "    for test in range(200):\n",
    "        \n",
    "        # dummy input\n",
    "        B, T = 1, 4\n",
    "        xe = torch.randn(B, T, C) # B,T,C for encoder\n",
    "        xd = torch.randn(B, T, C) # B,T,C for decoder\n",
    "\n",
    "        # set network into train/eval phase\n",
    "        phase = test % 2\n",
    "        model.train() if phase == 1 else model.eval()\n",
    "        \n",
    "        # predict mode\n",
    "        x = model(xe, xd)\n",
    "        y = torch.sigmoid(x)\n",
    "        pred = 1 if y.item() > 0.5 else 0\n",
    "        \n",
    "        # print(f\"{test} gt: {phase}, pred: {pred}, correct: {phase == pred}\")\n",
    "        corrects.append(float(phase == pred))\n",
    "\n",
    "    print(f\"test accuracy {torch.tensor(corrects).mean().item()*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9R9Zp4RP1Tu4",
    "outputId": "df01185a-0af5-4062-aac1-8ac09fbf3872"
   },
   "outputs": [],
   "source": [
    "# with dropout > 0.0 this should work, i.e. accuracy >> 50%\n",
    "model = TinyTransformer(dropout=0.2)\n",
    "train_model(model)\n",
    "eval_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1mqoPJHA0t5m",
    "outputId": "1b663064-cbff-4c2c-8527-703503af51aa"
   },
   "outputs": [],
   "source": [
    "# with dropout of 0 this should not work, i.e. accuracy ~= 50%\n",
    "model = TinyTransformer(dropout=0.0)\n",
    "train_model(model)\n",
    "eval_model(model)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
